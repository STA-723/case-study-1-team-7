---
title: "Investigating the Effects of DDE and PCB Exposure on Premature Delivery"
author: "Youngsoo Baek, Yunran Chen and Xiaojun Zheng"
fontsize: "11pt"
output:
  pdf_document: 
        latex_engine: xelatex
  html_document:
    keep_md: yes
    fig_caption: TRUE
header-includes:  
  \usepackage{float} 
  \floatplacement{figure}{H}
---

```{r setup, echo=F}
Longnecker <- readRDS("Longnecker.rds")
```


We study the association between exposure to Dichlorodiphenyldichloroethylene (DDE) and 12 Polychlorinated Biophenyl (PCB) members and premature delivery using observational study data. To address high correlation within PCB members that leads to unstable estimates in standard linear regression methods, we fit a logistic regression model that includes a weighted sum of standardized PCB levels instead of all PCB members, along with variable selection based on Lasso. We find evidence that higher exposure to both DDE and PCB is associated with higher odds of preterm delivery. Our result is relative robust in terms of influential points analysis and missing data imputation as suggested by the sensitivity analysis.

# Executive Summary
We study the association between exposure to Dichlorodiphenyldichloroethylene (DDE) and 12 Polychlorinated Biophenyl (PCB) members and premature delivery. Based on both binary and ordinal logistic regression models, we find evidence that higher exposure to both DDE and PCB is associated with higher odds of preterm delivery. Alternative modeling methods are discussed, across which we find consistent indication of DDE and PCB effect.

# Introduction
  

# Materials and Methods
We consider a class of linear regression models that can model the linear association between the probability of preterm delivery and predictors of interest on some suitable scale. In particular, we consider a logistic regression model, where the log-odds of premature delivery are modeled by a linear combination of DDE and PCB levels. While binary logistic regression is a standard method, we suffer loss of information on different delivery periods, which contains information about different levels of risk on the infants' health. An ordinal logistic regression can extend the response to have multiple ordered categoriesSpecifically, consider ordinal response $Y_i$ with four possible values, $k=1,2,3,4$, each corresponding to different categories of preterm delivery with increasing level of infant health risk. The model can be written as:
$${\rm Pr}(Y_i=k|\mathbf{x}_i) = {\rm logit}^{-1}(\alpha_{k} + \mathbf{x}_i^T\beta)\iff {\rm Odds}(Y_i=k|\mathbf{x}_i) = e^{\alpha_k} e^{\mathbf{x}_i^T\beta},$$
hence the odds of different preterm delivery risks are proportional to each other. The data exhibit some indication that the model assumption can be violated; i.e., DDE is observed to have stronger association with premature delivery, as we move from late preterm (nearly around 37-week threshold) to earlier delivery periods.

Before fitting the model, `r sum(Longnecker$gestational_age > 46)` observations for which delivery period was greater than 46 weeks were truncated at 46, based on assumption that extremely late delivery is impossible within the design of the study. Aside from measurements of DDE and 12 different PCB members, possible predictors to be adjusted for in the model included race, triglycerides, cholesterol, maternal age, smoking status, and three score variables for the subjects' income, education, and occupation. A complete case analysis adjusting for all these predictors excludes `r max(apply(Longnecker, 2, function(x) sum(is.na(x))))`, since income scores were not recorded for these patients. The ANOVA test of a logistic regression model that includes scores against a model that does not failed to reject the null of no significant effect, which offers justification of not including these in the final model. Thus, we have excluded only 1 subject for whom no PCB levels were measured. Finally, to account for high correlation between PCB members that can inflate variance of the estimated effects, we excluded all PCB levels and included the first principal component from the principal components analysis (PCA). Since this principal component is a weighted sum of all PCB members that are standardized to unit scale, it can be thought of as a proxy variable for "adjusted total" PCB levels. Direct interpretation of the effect magnitude based on regression estimates, however, is not feasible.  

# Results
We consider a class of linear regression models that can model the linear association between the probability of preterm delivery and predictors of interest on some suitable scale. In particular, we consider a logistic regression model for its good interpretability, where the log-odds of premature delivery are modeled by a linear combination of DDE and PCB levels, adjusted for observed demographic predictors including race, age, and smoking status, and triglyceride and cholesterol levels. While binary logistic regression is a standard method in modeling categorical response, we suffer a loss of information on different delivery periods, which does contain information about different levels of risk on the infants' health. Therefore, we bin the gestational ages to reflect increasing levels of risk prescribed by NIH and also consider an ordinal logistic regression with an assumption that the odds corresponding to each of these levels are proportional. We discuss and fit both models below. 

The main challenge is high correlation between multiple PCB variables. High correlation between predictors in the regression is known to inflate standard errors of the estimated effects, so our inference on not only the PCBs, but also for other predictors, can be severely limited by adjusting for all of them. Two approaches are discussed. We can either take a weighted sum of PCB variables after standardizing them to the same scale, and adjust for this single variable as a proxy for "aggregated" PCB levels. Alternatively, we can use variable selection methods to exclude PCB variables that are deemed unimportant for the model, and re-fit the regression based on the selected variables only.  For the first approach, we consider a two-stage modeling process. We apply Principal Component Analysis (PCA) and consider the first prcincipal component as a predictor in the binary/ordinary logistic regression. For the second approach, we apply Lasso to select representatives for PCBs and refit the binary/ordinary logistic regression.

(The second challenge is modeling heterogeneity across different centers, which is important since the data show they can have widely different racial distribution. Commonly, a modeler decides to estimate the effect of center as fixed or random. By the former, the heterogeneity of center is modeled as a shift in the average response adjusted for the other variables, and is estimated using subjects only within the center. By the latter, it is modeled as a variance component between different centers that have a common underlying distribution, and is estimated using partial information from all subjects. )

# Exploratory Data Analysis

As shown in figure , the dataset contains 2380 observations, with over 90% having missing values on predictor `albumin`, around 22% having missing values on socio-economic indices and 1 observation with missing value on PCB level. We drop the variable albumin and keep the complete cases in terms of remaining variables. A sensitivity analysis is also conducted to addressing the problem. As shown in Figure , strong correlations exist among PCBs, which suggest a severe multicollinearity may exist among PCBs. Figure shows a heterogeneity across centers exist. Since different centers have distinct demographic background (such as race and income as shown in Figure), we assume each center cannot borrow information from others and consider fix effects for 12 centers instead of random effect. For simplicity and save the degree of freedom, we do not consider the interaction between center and other predictors.

##### Main Results
Based on the binary logistic regression model, the significant predictors include DDE, proxy variable for PCB levels, triglyceride and cholesterol levels, along with mean shifts in centers (...) for which subjects are primarily of black race. Estimated model effects and their 95\% confidence intervals can be found in Table 1.

Most of the estimates have a direct scientific interpretation. For isntance, for a 1$\mu$g increase in DDE exposure, adjusted for all other variables, a mother has 0.8\% increased odds of premature delivery. Similarly, for a 1g/dL increase in triglyceride level, adjusted for other variables, a mother has 0.3\% increased odds of premature delivery. Such inference, however, is hampered for PCB, as discussed above. We do get an expected positive sign, indicating that PCB has effect oc increasing preterm delivery odds. Ordinal logistic model estimates mostly agree in the signs and magnitudes of the effects; however, cholesterol level is no longer considered a significant variable. 

(...Unfinished, but you can start adding LASSO results here...)

##### Sensitivity Analysis

As introduced above, the models selected by lasso regression involve the score variables about income, education and occupation. With about 20% missingness of score variables, we decided to impute the score variables, and refit the models to see whether there is signiciant change in our result. The method of imputation is called Predictive Mean Matching, which extracts the information from the observations without missing values, and use that to make assumption about the missing entry. Specially, for each missing entry, the method forms a set of complete observations with the predicted values that are close to the predcition of that, and randomly draws the observed value of an obervation to replace the missing value.

After refitting the logistic and ordinal logistic models using the imputed data, we observed that there is almost no change for the estimates of the coefficient for DDE, PCB_074 and PCB_153. At the same time, the confidence intervals for those coefficients narrow a little bit, which match up our intuition. Thus, our models generated based on a lasso selection seems to be robust. 


# Discussion

In addition to the frequentist methods we have described above, we also tried Bayesian methods, including Bayesian Model Averaging, Bayesian lasso, and a Hierarchical prior on center effects. Fortunately, we were able to select the same PCB covariates using Bayesian lasso, but the results for prediction is not satisfying. The confidence intervals for coefficients are large, and cover $0$, and mixing seems poor when we generated $10000$ samples. 


\clearpage

# Tables and Figures

...All tables and figures go here!...

```{r echo=FALSE}
library(arm)
library(corrplot)
library(naniar)
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(purrr)
library(lme4)
library(MASS)
library(ggmosaic)
library(mice)
library(ordinalNet)
```

```{r lasso, cache=T,echo=F, eval=T}
data=readRDS("Longnecker.rds")
data=data%>%dplyr::select(gestational_age,dde:cholesterol,center)
data_46=data%>%mutate(y=gestational_age)
data_46[data_46$y>46,"y"]=46
data_46=data_46%>%dplyr::select(-gestational_age,-albumin)%>%mutate(race=factor(race),center=factor(center))
data_46_wscore=data_46%>%na.omit()
set.seed(123)
train.data=data_46_wscore
x <- model.matrix(y~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$y<37,1,0)
train.data$y=y
cv.lasso2 <- cv.glmnet(x, y, alpha = 1,family="binomial")
# Fit the final model on the training data
model2 <- glmnet(x, y, alpha = 1,family="binomial",
                lambda = cv.lasso2$lambda[27])
```

```{r glmlasso, cache=T,echo=F, eval=T}
formula_lasso_w <- as.formula(
  "y ~ dde + pcb_074 + pcb_153 + 
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)
formula_lasso_o <- as.formula(
  "y ~ dde + 
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)
model21_w <- glm(formula_lasso_w, family = binomial, 
                   data = train.data)
model21_o <- glm(formula_lasso_o, family = binomial, 
                   data = train.data)
```

```{r cilassolog, include=FALSE}
lasso_coef <- coef(model21_w)
lasso_confint <- suppressMessages(confint(model21_w))
lassso_coefs <- cbind.data.frame(Mean = lasso_coef, lasso_confint)
```


```{r cache=T,echo=F}
train.data.ord=data_46_wscore
train.data.ord$y <- cut(train.data.ord$y,
                                       breaks = c(0, 31, 33, 36, 46), right = TRUE)
ord.x=model.matrix(y~.,data=train.data.ord)
#ordlasso.model <- ordinalNet(ord.x, train.data.ord$y, family="cumulative", link="logit",
#                   parallelTerms=TRUE, nonparallelTerms=FALSE,alpha=1)
#coef(ordlasso.model) # pcb_028,074,153,138
formula_lasso_w.ord <- as.formula(
  "y ~ dde + pcb_074 + pcb_153 +
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)
formula_lasso_o.ord <- as.formula(
  "y ~ dde + 
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)
model21_w.ord <- polr(formula_lasso_w.ord, 
                   data = train.data.ord)
model21_o.ord <- polr(formula_lasso_o.ord, 
                   data = train.data.ord)
```

```{r include=FALSE}
olasso_coefs <- -1 * coef(model21_w.ord)
olasso_confint <- suppressMessages(-1 * confint(model21_w.ord)[,c(2,1)])
olasso_coefs <- suppressMessages(cbind.data.frame(Mean = olasso_coefs, olasso_confint))
colnames(olasso_coefs) <- colnames(olasso_coefs)[c(1,3,2)]
```


```{r echo=FALSE}
data=readRDS("Longnecker.rds")
data=data%>%dplyr::select(gestational_age,dde:cholesterol,center)
data_46=data%>%mutate(y=gestational_age)
data_46[data_46$y>46,"y"]=46
data_46=data_46%>%dplyr::select(-gestational_age,-albumin)%>%mutate(race=factor(race),center=factor(center))

data_impute<- mice(data = data_46, m = 1, method = "pmm")
data <- complete(data_impute)

train.data=data

set.seed(123)
x <- model.matrix(y~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$y<37,1,0)
train.data$y=y
cv.lasso2 <- cv.glmnet(x, y, alpha = 1,family="binomial")
# Fit the final model on the training data
model2 <- glmnet(x, y, alpha = 1,family="binomial",
                lambda = cv.lasso2$lambda[27])


formula_lasso_w <- as.formula(
  "y ~ dde + pcb_074 + pcb_153 + 
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)

model21_w <- glm(formula_lasso_w, family = binomial, 
                   data = train.data)

lasso_coef_impute <- coef(model21_w)
lasso_confint_impute <- suppressMessages(confint(model21_w))
lassso_coefs_impute <- cbind.data.frame(Mean = lasso_coef, lasso_confint)


sensi_logistic<- data.frame(name = c("dde", "dde", "pcb074", "pcb074", "pcb153", "pcb153"), mean = c(lassso_coefs[2,1], lassso_coefs_impute[2,1], lassso_coefs[3,1], lassso_coefs_impute[3,1], lassso_coefs[4,1], lassso_coefs_impute[4,1]), lower = c(lassso_coefs[2,2], lassso_coefs_impute[2,2], lassso_coefs[3,2], lassso_coefs_impute[3,2], lassso_coefs[4,2], lassso_coefs_impute[4,2]), higher = c(lassso_coefs[2,3], lassso_coefs_impute[2,3], lassso_coefs[3,3], lassso_coefs_impute[3,3], lassso_coefs[4,3], lassso_coefs_impute[4,3]), type = c("missing", "impute", "missing", "impute", "missing", "impute"))

ggplot(sensi_logistic, aes(x = name, y = mean,ymin = lower, ymax =higher, 
               color=type)) +
    geom_point(position=position_dodge(width=0.5), size = 2.5) + 
    geom_linerange(position=position_dodge(width=0.5), size =0.5) +  theme_bw() +
    labs(colour="Type", y="Log Odds", x="") + 
    theme(plot.title = element_text(size = 12,face="bold" )) + 
    theme(axis.title=element_text(size="12") ,axis.text=element_text(size=12)) 

```

```{r ordlasso, cache=T,echo=F}
data=readRDS("Longnecker.rds")
data=data%>%dplyr::select(gestational_age,dde:cholesterol,center)
data_46=data%>%mutate(y=gestational_age)
data_46[data_46$y>46,"y"]=46
data_46=data_46%>%dplyr::select(-albumin)%>%mutate(race=factor(race),center=factor(center))

data_impute<- mice(data = data_46, m = 1, method = "pmm")
data <- complete(data_impute)
train.data.ord<- data
train.data.ord$y <- cut(train.data.ord$gestational_age,
                                       breaks = c(0, 31, 33, 36, 46), right = TRUE)
ord.x=model.matrix(y~.,data=train.data.ord)
#ordlasso.model <- ordinalNet(ord.x, train.data.ord$y, family="cumulative", link="logit",
#                   parallelTerms=TRUE, nonparallelTerms=FALSE,alpha=1)
#coef(ordlasso.model) # pcb_028,074,153,138
formula_lasso_w.ord <- as.formula(
  "y ~ dde + pcb_074 + pcb_153 +
  triglycerides + race + maternal_age + smoking_status + cholesterol + center + score_education + score_income + score_occupation"
)

model21_w.ord <- polr(formula_lasso_w.ord, 
                   data = train.data.ord)

```

```{r cilassoord, include=FALSE}
olasso_coefs_impute <- -1 * coef(model21_w.ord)
olasso_confint_impute <- suppressMessages(-1 * confint(model21_w.ord)[,c(2,1)])
olasso_coefs_impute <- suppressMessages(cbind.data.frame(Mean = olasso_coefs_impute, olasso_confint_impute))
colnames(olasso_coefs_impute) <- colnames(olasso_coefs_impute)[c(1,3,2)]
```

```{r}
sensi_logistic<- data.frame(name = c("dde", "dde", "pcb074", "pcb074", "pcb153", "pcb153"), mean = c(olasso_coefs[1,1], olasso_coefs_impute[1,1], olasso_coefs[2,1], olasso_coefs_impute[2,1], olasso_coefs[3,1], olasso_coefs_impute[3,1]), lower = c(olasso_coefs[1,2], olasso_coefs_impute[1,2], olasso_coefs[2,2], olasso_coefs_impute[2,2], olasso_coefs[3,2], olasso_coefs_impute[3,2]), higher = c(olasso_coefs[1,3], olasso_coefs_impute[1,3], olasso_coefs[2,3], olasso_coefs_impute[2,3], olasso_coefs[3,3], olasso_coefs_impute[3,3]), type = c("missing", "impute", "missing", "impute", "missing", "impute"))

ggplot(sensi_logistic, aes(x = name, y = mean,ymin = lower, ymax =higher, 
               color=type)) +
    geom_point(position=position_dodge(width=0.5), size = 2.5) + 
    geom_linerange(position=position_dodge(width=0.5), size =0.5) +  theme_bw() +
    labs(colour="Type", y="Log Odds", x="") + 
    theme(plot.title = element_text(size = 12,face="bold" )) + 
    theme(axis.title=element_text(size="12") ,axis.text=element_text(size=12)) 


```


\clearpage

# Appendix

# References
...If there are any...
